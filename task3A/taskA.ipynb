{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_A_PATH = \"dataset3a/\"\n",
    "SPLIT_DATA_PATH_A = \"data_a/\"\n",
    "PATH_TRAIN_LR = \"data_a/train/LR\"\n",
    "PATH_TRAIN_HR = \"data_a/train/HR\"\n",
    "PATH_VAL_LR = \"data_a/val/LR\"\n",
    "PATH_VAL_HR = \"data_a/val/HR\"\n",
    "\n",
    "DATA_B_PATH = \"dataset3b/\"\n",
    "SPLIT_DATA_PATH_B = \"data_b/\"\n",
    "PATH_TRAIN_LR_B = \"data_b/train/LR\"\n",
    "PATH_TRAIN_HR_B = \"data_b/train/HR\"\n",
    "PATH_VAL_LR_B = \"data_b/val/LR\"\n",
    "PATH_VAL_HR_B = \"data_b/val/HR\"\n",
    "\n",
    "SAVED_MODEL_PATH = \"task3A/equiformer_best_0.pth\"\n",
    "\n",
    "LR = \"LR\"\n",
    "HR = \"HR\"\n",
    "TRAIN = \"train\"\n",
    "VAL = \"val\"\n",
    "\n",
    "WANDB_USERNAME = \"samkitshah1262-warner-bros-discovery\"\n",
    "WANDB_PROJECT = \"ml4sci-superres\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBLogger:\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.run = wandb.init(\n",
    "            project=config['wandb_project'],\n",
    "            entity=config['wandb_entity'],\n",
    "            config=config,\n",
    "            tags=config.get('tags', ['super-resolution', 'lensing']),\n",
    "            dir=str(Path.cwd())\n",
    "        )\n",
    "\n",
    "        wandb.watch(\n",
    "            model,\n",
    "            log='all',\n",
    "            log_freq=config.get('log_interval', 50),\n",
    "            log_graph=True\n",
    "        )\n",
    "        \n",
    "    def log_metrics(self, metrics, step=None, commit=True):\n",
    "        wandb.log(metrics, step=step, commit=commit)\n",
    "        \n",
    "    def log_images(self, lr, sr, hr, caption=\"LR/SR/HR Comparison\"):\n",
    "        # Resize LR to match HR/SR dimensions\n",
    "        lr_upscaled = F.interpolate(lr, scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        # Denormalize images\n",
    "        lr_upscaled = (lr_upscaled * 0.5 + 0.5).clamp(0, 1)\n",
    "        sr = (sr * 0.5 + 0.5).clamp(0, 1)\n",
    "        hr = (hr * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "        grid = torch.cat([lr_upscaled, sr, hr], dim=-1)  # Concatenate along width\n",
    "\n",
    "        images = wandb.Image(grid, caption=caption)\n",
    "        wandb.log({\"Examples\": images})\n",
    "        \n",
    "    def log_model(self, model_path, metadata=None):\n",
    "        artifact = wandb.Artifact(\n",
    "            name=f\"model-{wandb.run.id}\",\n",
    "            type=\"model\",\n",
    "            description=\"Equiformer super-resolution model\",\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        artifact.add_file(model_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        \n",
    "    def finish(self):\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = DATA_A_PATH # Change this to your dataset path\n",
    "lr_dir = os.path.join(dataset_root, LR)\n",
    "hr_dir = os.path.join(dataset_root, HR)\n",
    "\n",
    "# Define output directories\n",
    "output_root = SPLIT_DATA_PATH_A  # Change this to your desired output path\n",
    "train_lr_dir = os.path.join(output_root, \"train\", LR)\n",
    "train_hr_dir = os.path.join(output_root, \"train\", HR)\n",
    "val_lr_dir = os.path.join(output_root, \"val\", LR)\n",
    "val_hr_dir = os.path.join(output_root, \"val\", HR)\n",
    "\n",
    "# Create train/val directories\n",
    "for dir_path in [train_lr_dir, train_hr_dir, val_lr_dir, val_hr_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Get all sample file names\n",
    "lr_files = sorted(os.listdir(lr_dir))  # Ensure matching order\n",
    "hr_files = sorted(os.listdir(hr_dir))\n",
    "\n",
    "# Ensure pairs match\n",
    "assert len(lr_files) == len(hr_files), \"Mismatch in LR and HR files!\"\n",
    "\n",
    "# Split dataset (90% train, 10% val)\n",
    "train_lr, val_lr, train_hr, val_hr = train_test_split(\n",
    "    lr_files, hr_files, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Function to copy files\n",
    "def move_files(files, src_dir, dest_dir):\n",
    "    for file in files:\n",
    "        shutil.copy(os.path.join(src_dir, file), os.path.join(dest_dir, file))\n",
    "\n",
    "# Move files to respective directories\n",
    "move_files(train_lr, lr_dir, train_lr_dir)\n",
    "move_files(train_hr, hr_dir, train_hr_dir)\n",
    "move_files(val_lr, lr_dir, val_lr_dir)\n",
    "move_files(val_hr, hr_dir, val_hr_dir)\n",
    "\n",
    "print(\"Dataset split completed successfully!\")\n",
    "\n",
    "dataset_root = DATA_B_PATH # Change this to your dataset path\n",
    "lr_dir = os.path.join(dataset_root, LR)\n",
    "hr_dir = os.path.join(dataset_root, HR)\n",
    "\n",
    "# Define output directories\n",
    "output_root = SPLIT_DATA_PATH_B  # Change this to your desired output path\n",
    "train_lr_dir = os.path.join(output_root, \"train\", LR)\n",
    "train_hr_dir = os.path.join(output_root, \"train\", HR)\n",
    "val_lr_dir = os.path.join(output_root, \"val\", LR)\n",
    "val_hr_dir = os.path.join(output_root, \"val\", HR)\n",
    "\n",
    "# Create train/val directories\n",
    "for dir_path in [train_lr_dir, train_hr_dir, val_lr_dir, val_hr_dir]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Get all sample file names\n",
    "lr_files = sorted(os.listdir(lr_dir))  # Ensure matching order\n",
    "hr_files = sorted(os.listdir(hr_dir))\n",
    "\n",
    "# Ensure pairs match\n",
    "assert len(lr_files) == len(hr_files), \"Mismatch in LR and HR files!\"\n",
    "\n",
    "# Split dataset (90% train, 10% val)\n",
    "train_lr, val_lr, train_hr, val_hr = train_test_split(\n",
    "    lr_files, hr_files, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Function to copy files\n",
    "def move_files(files, src_dir, dest_dir):\n",
    "    for file in files:\n",
    "        shutil.copy(os.path.join(src_dir, file), os.path.join(dest_dir, file))\n",
    "\n",
    "# Move files to respective directories\n",
    "move_files(train_lr, lr_dir, train_lr_dir)\n",
    "move_files(train_hr, hr_dir, train_hr_dir)\n",
    "move_files(val_lr, lr_dir, val_lr_dir)\n",
    "move_files(val_hr, hr_dir, val_hr_dir)\n",
    "\n",
    "print(\"Dataset split completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two image tensors\"\"\"\n",
    "    img1 = img1.detach().cpu().numpy().transpose(0,2,3,1)\n",
    "    img2 = img2.detach().cpu().numpy().transpose(0,2,3,1)\n",
    "    return np.mean([psnr(im1, im2, data_range=1.0) \n",
    "                   for im1, im2 in zip(img1, img2)])\n",
    "\n",
    "def calculate_ssim(img1, img2, data_range=1.0, eps=1e-8):\n",
    "    \"\"\"Numerically stable SSIM calculation for grayscale images\"\"\"\n",
    "    # Input validation and clamping\n",
    "    img1 = torch.clamp(img1, -data_range, data_range).detach()\n",
    "    img2 = torch.clamp(img2, -data_range, data_range).detach()\n",
    "    \n",
    "    # Convert to numpy with double precision\n",
    "    img1_np = img1.cpu().numpy().squeeze(1).astype(np.float64)\n",
    "    img2_np = img2.cpu().numpy().squeeze(1).astype(np.float64)\n",
    "    \n",
    "    ssim_values = []\n",
    "    \n",
    "    for i in range(img1_np.shape[0]):\n",
    "        im1 = img1_np[i]\n",
    "        im2 = img2_np[i]\n",
    "        \n",
    "        try:\n",
    "            # Dynamic window size selection with safety checks\n",
    "            min_dim = min(im1.shape)\n",
    "            win_size = min(7, min_dim - 1 if min_dim % 2 == 0 else min_dim)\n",
    "            win_size = max(3, win_size)\n",
    "            \n",
    "            # Calculate SSIM with stability parameters\n",
    "            ssim_val = ssim(\n",
    "                im1, im2,\n",
    "                data_range=data_range,\n",
    "                win_size=win_size,\n",
    "                channel_axis=None,\n",
    "                gaussian_weights=True,\n",
    "                sigma=1.5,\n",
    "                use_sample_covariance=False\n",
    "            )\n",
    "            \n",
    "            # Handle potential NaN/Inf\n",
    "            if np.isnan(ssim_val) or np.isinf(ssim_val):\n",
    "                raise ValueError(\"Invalid SSIM value\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"SSIM calculation failed for image {i}: {str(e)}\")\n",
    "            ssim_val = -1  # Sentinel value for failures\n",
    "            \n",
    "        ssim_values.append(ssim_val)\n",
    "    \n",
    "    # Filter out failed calculations\n",
    "    valid_ssim = [v for v in ssim_values if v >= 0]\n",
    "    \n",
    "    return np.mean(valid_ssim) if valid_ssim else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dim': 64,\n",
    "    'num_blocks': 8,\n",
    "    'num_heads': 4,\n",
    "    'upscale': 2,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'use_amp': True,\n",
    "    'train_lr_dir': PATH_TRAIN_LR,\n",
    "    'train_hr_dir': PATH_TRAIN_HR,\n",
    "    'val_lr_dir': PATH_VAL_LR,\n",
    "    'val_hr_dir': PATH_VAL_HR,\n",
    "    'transform': transforms.Compose([\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "    ]),\n",
    "    'wandb_project': WANDB_PROJECT,\n",
    "    'wandb_entity': WANDB_USERNAME,\n",
    "    'tags': ['gsoc2025', 'diffilens'],\n",
    "    'log_interval': 50,\n",
    "    'sample_interval': 200,\n",
    "    'architecture': 'Equiformer'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LensDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Strong lensing dataset handler for .npy files\"\"\"\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_files = sorted(os.listdir(lr_dir))  # Ensure order\n",
    "        self.hr_files = sorted(os.listdir(hr_dir))\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.lr_files) == len(self.hr_files), \"Mismatch in LR and HR files!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lr = np.load(os.path.join(self.lr_dir, self.lr_files[idx]))\n",
    "        hr = np.load(os.path.join(self.hr_dir, self.hr_files[idx]))\n",
    "\n",
    "\n",
    "        lr = torch.tensor(lr, dtype=torch.float32)\n",
    "        hr = torch.tensor(hr, dtype=torch.float32) \n",
    "\n",
    "        if self.transform:\n",
    "            lr = self.transform(lr)\n",
    "            hr = self.transform(hr)\n",
    "\n",
    "        return lr, hr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LensDataPreprocessor:\n",
    "    def __init__(self, crop_size=75, scale_factor=2):\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip()\n",
    "            ]),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomCrop(crop_size*scale_factor),\n",
    "            transforms.Lambda(lambda x: self._degrade(x, scale_factor)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size*scale_factor),\n",
    "            transforms.Lambda(lambda x: self._degrade(x, scale_factor)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def _degrade(self, hr, scale_factor):\n",
    "        lr_size = (hr.size[1]//scale_factor, hr.size[0]//scale_factor)\n",
    "        return hr.resize(lr_size, Image.BICUBIC)\n",
    "\n",
    "    def get_transforms(self):\n",
    "        return {\n",
    "            'train': PairedTransform(self.train_transform),\n",
    "            'val': PairedTransform(self.val_transform)\n",
    "        }\n",
    "\n",
    "class PairedTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __call__(self, hr):\n",
    "        lr = self.transform(hr)\n",
    "        hr = self.transform(hr)\n",
    "        return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self, layers=[2, 7, 14]): \n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True).features \n",
    "        self.layers = layers\n",
    "        self.vgg_layers = nn.ModuleList([vgg[i] for i in layers])\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False \n",
    "        self.vgg = vgg.eval()\n",
    "        \n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        sr = sr.repeat(1, 3, 1, 1)  # (B, 1, H, W) → (B, 3, H, W)\n",
    "        hr = hr.repeat(1, 3, 1, 1)\n",
    "        \n",
    "        sr_features = self.vgg(sr)\n",
    "        hr_features = self.vgg(hr)\n",
    "        return F.l1_loss(sr_features, hr_features)\n",
    "    \n",
    "\n",
    "class PhysicsConstrainedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.1, beta=0.01, device=device):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Weight for mass conservation\n",
    "        self.beta = beta    # Weight for lensing equation\n",
    "        self.device = device\n",
    "        \n",
    "        # Sobel filters for gradient calculation\n",
    "        self.sobel_x = torch.tensor([[[[1, 0, -1], \n",
    "                                      [2, 0, -2], \n",
    "                                      [1, 0, -1]]]], dtype=torch.float32, device=device)\n",
    "        self.sobel_y = torch.tensor([[[[1, 2, 1], \n",
    "                                      [0, 0, 0], \n",
    "                                      [-1, -2, -1]]]], dtype=torch.float32, device=device)\n",
    "\n",
    "    def gradient(self, img):\n",
    "        \"\"\"Calculate image gradients using Sobel operators\"\"\"\n",
    "        grad_x = F.conv2d(img, self.sobel_x, padding=1)\n",
    "        grad_y = F.conv2d(img, self.sobel_y, padding=1)\n",
    "        return grad_x, grad_y\n",
    "\n",
    "    def laplacian(self, img):\n",
    "        \"\"\"Calculate image Laplacian\"\"\"\n",
    "        kernel = torch.tensor([[[[0, 1, 0], \n",
    "                               [1, -4, 1], \n",
    "                               [0, 1, 0]]]], dtype=torch.float32, device=self.device)\n",
    "        return F.conv2d(img, kernel, padding=1)\n",
    "\n",
    "    def mass_conservation_loss(self, sr, hr):\n",
    "        \"\"\"\n",
    "        Enforce conservation of total flux/mass between \n",
    "        LR upscaled and SR reconstruction\n",
    "        \"\"\"\n",
    "        lr_upscaled = F.interpolate(sr, scale_factor=0.5, mode='bicubic')\n",
    "        return F.mse_loss(lr_upscaled.mean(dim=(2,3)), sr.mean(dim=(2,3)))\n",
    "\n",
    "    def lensing_equation_loss(self, sr):\n",
    "        \"\"\"\n",
    "        Enforce weak lensing approximation:\n",
    "        ∇²ψ = 2κ where ψ is lensing potential, κ is convergence\n",
    "        Approximated using image gradients and Laplacian\n",
    "        \"\"\"\n",
    "        grad_x, grad_y = self.gradient(sr)\n",
    "        lap = self.laplacian(sr)\n",
    "        \n",
    "        # Simulated convergence (κ) from image intensity\n",
    "        kappa = sr.mean(dim=1, keepdim=True)  # Simplified assumption\n",
    "        \n",
    "        # Lensing equation residual\n",
    "        residual = lap - 2*kappa\n",
    "        return torch.mean(residual**2)\n",
    "\n",
    "    def forward(self, sr, hr):\n",
    "        base_loss = F.l1_loss(sr, hr)\n",
    "        mass_loss = self.mass_conservation_loss(sr, hr)\n",
    "        lens_loss = self.lensing_equation_loss(sr)\n",
    "        \n",
    "        return base_loss + self.alpha*mass_loss + self.beta*lens_loss\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, weight=1.0):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        h_variation = torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).sum()\n",
    "        w_variation = torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).sum()\n",
    "        return self.weight * (h_variation + w_variation) / (batch_size * channels * height * width)\n",
    "\n",
    "class HybridLoss(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super().__init__()\n",
    "        self.physics = PhysicsConstrainedLoss(device=device)\n",
    "        self.vgg = VGGPerceptualLoss().to(device)\n",
    "        self.tv = TVLoss()\n",
    "    def forward(self, sr, hr):\n",
    "        return (0.7*self.physics(sr, hr) + \n",
    "                0.2*self.vgg(sr, hr) + \n",
    "                0.1*self.tv(sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, groups=4):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                            padding=kernel_size//2, groups=groups)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.view(b, self.groups, c//self.groups, h, w)\n",
    "        x = self.conv(x.reshape(b, c, h, w))\n",
    "        return x.view(b, -1, h, w)\n",
    "\n",
    "class EquivariantAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=4, groups=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.groups = groups\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = GroupConv(dim, dim*3, 1, groups=groups)\n",
    "        self.proj = GroupConv(dim, dim, 1, groups=groups)\n",
    "        \n",
    "        self.norm = nn.GroupNorm(groups, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        qkv = self.qkv(self.norm(x)).chunk(3, dim=1)\n",
    "        \n",
    "        q, k, v = map(lambda t: t.view(B, self.num_heads, C // self.num_heads, H, W), qkv)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        x = (attn @ v).reshape(B, C, H, W)\n",
    "        x = self.proj(x)\n",
    "        return x + x\n",
    "\n",
    "class EquivariantFFN(nn.Module):\n",
    "    def __init__(self, dim, expansion=4, groups=4):\n",
    "        super().__init__()\n",
    "        hidden_dim = dim * expansion\n",
    "        self.net = nn.Sequential(\n",
    "            GroupConv(dim, hidden_dim, 1, groups=groups),\n",
    "            nn.GELU(),\n",
    "            GroupConv(hidden_dim, dim, 1, groups=groups)\n",
    "        )\n",
    "        self.norm = nn.GroupNorm(groups, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(self.norm(x)) + x\n",
    "\n",
    "class EquiformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, groups=4, mlp_expansion=4):\n",
    "        super().__init__()\n",
    "        self.attn = EquivariantAttention(dim, num_heads, groups)\n",
    "        self.ffn = EquivariantFFN(dim, mlp_expansion, groups)\n",
    "        self.norm1 = nn.GroupNorm(groups, dim)\n",
    "        self.norm2 = nn.GroupNorm(groups, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.attn(self.norm1(x)) + x\n",
    "        x = self.ffn(self.norm2(x)) + x\n",
    "        return x\n",
    "\n",
    "class Equiformer(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, dim=64, \n",
    "                 num_blocks=8, num_heads=4, groups=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Conv2d(in_channels, dim, 3, padding=1)\n",
    "        \n",
    "        self.blocks = nn.Sequential(*[\n",
    "            EquiformerBlock(dim, num_heads, groups)\n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.upsampler = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim * 4, 3, padding=1),  # 2² = 4 channels for PixelShuffle\n",
    "            nn.PixelShuffle(2),  # 2× upscale\n",
    "            nn.Conv2d(dim, out_channels, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_low = x\n",
    "        x = self.embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.upsampler(x)\n",
    "        return x + F.interpolate(x_low, scale_factor=2, mode='bilinear')  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, \n",
    "                 optimizer, criterion, device, config,\n",
    "                 scheduler=None, use_amp=True):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.scaler = GradScaler(device, enabled=use_amp)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.logger = WandBLogger(config, model)\n",
    "        self.log_interval = config.get('log_interval', 50)\n",
    "        self.sample_interval = config.get('sample_interval', 200)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0 \n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        mse_values = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (lr, hr) in enumerate(self.train_loader):\n",
    "            lr = lr.to(self.device, non_blocking=True)\n",
    "            hr = hr.to(self.device, non_blocking=True)\n",
    "            \n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with autocast(enabled=self.scaler.is_enabled()):\n",
    "                outputs = self.model(lr)\n",
    "                loss = self.criterion(outputs, hr)\n",
    "                \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_mse = F.mse_loss(outputs, hr).item()\n",
    "            batch_psnr = calculate_psnr(outputs, hr)\n",
    "            batch_ssim = calculate_ssim(outputs, hr)\n",
    "            psnr_values.append(batch_psnr)\n",
    "            ssim_values.append(batch_ssim)\n",
    "            mse_values.append(batch_mse)\n",
    "\n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                self.logger.log_metrics({\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/batch_mse\": batch_mse,\n",
    "                    \"train/batch_psnr\": batch_psnr,\n",
    "                    \"train/batch_ssim\": batch_ssim,\n",
    "                    \"lr\": self.optimizer.param_groups[0]['lr']\n",
    "                }, commit=False)\n",
    "                \n",
    "            if batch_idx % self.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    self.logger.log_images(lr[:1], outputs[:1], hr[:1])\n",
    "                \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        avg_ssim = np.mean(ssim_values)\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        self.logger.log_metrics({\n",
    "            \"epoch\": epoch,\n",
    "            \"train/avg_loss\": avg_loss,\n",
    "            \"train/epoch_mse\": avg_mse,\n",
    "            \"train/avg_psnr\": avg_psnr,\n",
    "            \"train/avg_ssim\": avg_ssim,\n",
    "            \"epoch_time\": epoch_time\n",
    "        })\n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        mse_values = []\n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for lr, hr in self.val_loader:\n",
    "            lr = lr.to(self.device, non_blocking=True)\n",
    "            hr = hr.to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(lr)\n",
    "            loss = self.criterion(outputs, hr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            mse_values.append(F.mse_loss(outputs, hr).item())\n",
    "            psnr_values.append(calculate_psnr(outputs, hr))\n",
    "            ssim_values.append(calculate_ssim(outputs, hr))\n",
    "            \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        avg_ssim = np.mean(ssim_values)\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        self.logger.log_metrics({\n",
    "            \"val/loss\": avg_loss,\n",
    "            \"val/mse\": avg_mse,\n",
    "            \"val/psnr\": avg_psnr,\n",
    "            \"val/ssim\": avg_ssim,\n",
    "            \"epoch_time\": epoch_time\n",
    "        })\n",
    "\n",
    "        self.logger.log_metrics({\n",
    "            \"val_output_dist\": wandb.Histogram(outputs.cpu().numpy())\n",
    "        })\n",
    "\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, 'equiformer_best.pth')\n",
    "            print(\"Saved best model!\")\n",
    "            self.logger.log_model('equiformer_best.pth', {\n",
    "                'epoch': epoch,\n",
    "                'val_loss': avg_loss,\n",
    "                'val_psnr': avg_psnr\n",
    "            })\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(avg_loss)\n",
    "            \n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    print(device)\n",
    "    model = Equiformer(\n",
    "        dim=config['dim'],\n",
    "        num_blocks=config['num_blocks'],\n",
    "        num_heads=config['num_heads'],\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    criterion = HybridLoss(device=device)\n",
    "    \n",
    "    # preprocessor = LensDataPreprocessor(crop_size=75)\n",
    "    # transforms = preprocessor.get_transforms()\n",
    "\n",
    "    train_dataset = LensDataset(\n",
    "        lr_dir=config['train_lr_dir'],\n",
    "        hr_dir=config['train_hr_dir'],\n",
    "        transform=config['transform']\n",
    "    )\n",
    "    \n",
    "    val_dataset = LensDataset(\n",
    "        lr_dir=config['val_lr_dir'],\n",
    "        hr_dir=config['val_hr_dir'],\n",
    "        transform=config['transform']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        config=config,\n",
    "        use_amp=config['use_amp']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, config['epochs'] + 1):\n",
    "            train_loss = trainer.train_epoch(epoch)\n",
    "            val_loss = trainer.validate(epoch)\n",
    "\n",
    "            # Early stopping\n",
    "            if (epoch - trainer.best_epoch) > config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    finally:\n",
    "        trainer.logger.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
