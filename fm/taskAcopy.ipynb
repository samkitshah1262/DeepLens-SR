{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_A_PATH = \"dataset3a/\"\n",
    "SPLIT_DATA_PATH_A = \"data_a/\"\n",
    "PATH_TRAIN_LR = \"../sr/data_a/train/LR\"\n",
    "PATH_TRAIN_HR = \"../sr/data_a/train/HR\"\n",
    "PATH_VAL_LR = \"../sr/data_a/val/LR\"\n",
    "PATH_VAL_HR = \"../sr/data_a/val/HR\"\n",
    "\n",
    "DATA_B_PATH = \"dataset3b/\"\n",
    "SPLIT_DATA_PATH_B = \"data_b/\"\n",
    "PATH_TRAIN_LR_B = \"data_b/train/LR\"\n",
    "PATH_TRAIN_HR_B = \"data_b/train/HR\"\n",
    "PATH_VAL_LR_B = \"data_b/val/LR\"\n",
    "PATH_VAL_HR_B = \"data_b/val/HR\"\n",
    "\n",
    "SAVED_MODEL_PATH = \"best_mae_model2.pth\"\n",
    "\n",
    "LR = \"LR\"\n",
    "HR = \"HR\"\n",
    "TRAIN = \"train\"\n",
    "VAL = \"val\"\n",
    "\n",
    "WANDB_USERNAME = \"samkitshah1262-warner-bros-discovery\"\n",
    "WANDB_PROJECT = \"deeplens-foundational\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandBLogger:\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.run = wandb.init(\n",
    "            project=config['wandb_project'],\n",
    "            entity=config['wandb_entity'],\n",
    "            config=config,\n",
    "            tags=config.get('tags', ['super-resolution', 'lensing']),\n",
    "            dir=str(Path.cwd())\n",
    "        )\n",
    "\n",
    "        wandb.watch(\n",
    "            model,\n",
    "            log='all',\n",
    "            log_freq=config.get('log_interval', 50),\n",
    "            log_graph=True\n",
    "        )\n",
    "        \n",
    "    def log_metrics(self, metrics, step=None, commit=True):\n",
    "        wandb.log(metrics, step=step, commit=commit)\n",
    "        \n",
    "    def log_images(self, lr, sr, hr, caption=\"LR/SR/HR Comparison\"):\n",
    "        # Resize LR to match HR/SR dimensions\n",
    "        lr_upscaled = F.interpolate(lr, scale_factor=2, mode='bilinear')\n",
    "        \n",
    "        # Denormalize images\n",
    "        lr_upscaled = (lr_upscaled * 0.5 + 0.5).clamp(0, 1)\n",
    "        sr = (sr * 0.5 + 0.5).clamp(0, 1)\n",
    "        hr = (hr * 0.5 + 0.5).clamp(0, 1)\n",
    "\n",
    "        grid = torch.cat([lr_upscaled, sr, hr], dim=-1)  # Concatenate along width\n",
    "\n",
    "        images = wandb.Image(grid, caption=caption)\n",
    "        wandb.log({\"Examples\": images})\n",
    "        \n",
    "    def log_model(self, model_path, metadata=None):\n",
    "        artifact = wandb.Artifact(\n",
    "            name=f\"model-{wandb.run.id}\",\n",
    "            type=\"model\",\n",
    "            description=\"Equiformer super-resolution model\",\n",
    "            metadata=metadata or {}\n",
    "        )\n",
    "        artifact.add_file(model_path)\n",
    "        wandb.log_artifact(artifact)\n",
    "        \n",
    "    def finish(self):\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two image tensors\"\"\"\n",
    "    img1 = img1.detach().cpu().numpy().transpose(0,2,3,1)\n",
    "    img2 = img2.detach().cpu().numpy().transpose(0,2,3,1)\n",
    "    return np.mean([psnr(im1, im2, data_range=1.0) \n",
    "                   for im1, im2 in zip(img1, img2)])\n",
    "\n",
    "def calculate_ssim(img1, img2, data_range=1.0, eps=1e-8):\n",
    "    \"\"\"Numerically stable SSIM calculation for grayscale images\"\"\"\n",
    "    # Input validation and clamping\n",
    "    img1 = torch.clamp(img1, -data_range, data_range).detach()\n",
    "    img2 = torch.clamp(img2, -data_range, data_range).detach()\n",
    "    \n",
    "    # Convert to numpy with double precision\n",
    "    img1_np = img1.cpu().numpy().squeeze(1).astype(np.float64)\n",
    "    img2_np = img2.cpu().numpy().squeeze(1).astype(np.float64)\n",
    "    \n",
    "    ssim_values = []\n",
    "    \n",
    "    for i in range(img1_np.shape[0]):\n",
    "        im1 = img1_np[i]\n",
    "        im2 = img2_np[i]\n",
    "        \n",
    "        try:\n",
    "            # Dynamic window size selection with safety checks\n",
    "            min_dim = min(im1.shape)\n",
    "            win_size = min(7, min_dim - 1 if min_dim % 2 == 0 else min_dim)\n",
    "            win_size = max(3, win_size)\n",
    "            \n",
    "            # Calculate SSIM with stability parameters\n",
    "            ssim_val = ssim(\n",
    "                im1, im2,\n",
    "                data_range=data_range,\n",
    "                win_size=win_size,\n",
    "                channel_axis=None,\n",
    "                gaussian_weights=True,\n",
    "                sigma=1.5,\n",
    "                use_sample_covariance=False\n",
    "            )\n",
    "            \n",
    "            # Handle potential NaN/Inf\n",
    "            if np.isnan(ssim_val) or np.isinf(ssim_val):\n",
    "                raise ValueError(\"Invalid SSIM value\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"SSIM calculation failed for image {i}: {str(e)}\")\n",
    "            ssim_val = -1  # Sentinel value for failures\n",
    "            \n",
    "        ssim_values.append(ssim_val)\n",
    "    \n",
    "    # Filter out failed calculations\n",
    "    valid_ssim = [v for v in ssim_values if v >= 0]\n",
    "    \n",
    "    return np.mean(valid_ssim) if valid_ssim else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dim': 64,\n",
    "    'num_blocks': 8,\n",
    "    'num_heads': 4,\n",
    "    'upscale': 2,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 100,\n",
    "    'patience': 10,\n",
    "    'use_amp': True,\n",
    "    'train_lr_dir': PATH_TRAIN_LR,\n",
    "    'train_hr_dir': PATH_TRAIN_HR,\n",
    "    'val_lr_dir': PATH_VAL_LR,\n",
    "    'val_hr_dir': PATH_VAL_HR,\n",
    "    'transform': transforms.Compose([\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]) \n",
    "    ]),\n",
    "    'wandb_project': WANDB_PROJECT,\n",
    "    'wandb_entity': WANDB_USERNAME,\n",
    "    'tags': ['gsoc2025', 'diffilens'],\n",
    "    'log_interval': 50,\n",
    "    'sample_interval': 200,\n",
    "    'architecture': 'Equiformer'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LensDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Strong lensing dataset handler for .npy files\"\"\"\n",
    "\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_files = sorted(os.listdir(lr_dir))  # Ensure order\n",
    "        self.hr_files = sorted(os.listdir(hr_dir))\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        assert len(self.lr_files) == len(self.hr_files), \"Mismatch in LR and HR files!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lr = np.load(os.path.join(self.lr_dir, self.lr_files[idx]))\n",
    "        hr = np.load(os.path.join(self.hr_dir, self.hr_files[idx]))\n",
    "\n",
    "        if len(lr.shape) == 2:\n",
    "            lr = lr[None, ...]\n",
    "        if len(hr.shape) == 2:\n",
    "            hr = hr[None, ...]\n",
    "\n",
    "        lr = torch.tensor(lr, dtype=torch.float32)\n",
    "        hr = torch.tensor(hr, dtype=torch.float32) \n",
    "\n",
    "        if self.transform:\n",
    "            lr = self.transform(lr)\n",
    "            hr = self.transform(hr)\n",
    "\n",
    "        return lr, hr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LensDataPreprocessor:\n",
    "    def __init__(self, crop_size=75, scale_factor=2):\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip()\n",
    "            ]),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomCrop(crop_size*scale_factor),\n",
    "            transforms.Lambda(lambda x: self._degrade(x, scale_factor)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.CenterCrop(crop_size*scale_factor),\n",
    "            transforms.Lambda(lambda x: self._degrade(x, scale_factor)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def _degrade(self, hr, scale_factor):\n",
    "        lr_size = (hr.size[1]//scale_factor, hr.size[0]//scale_factor)\n",
    "        return hr.resize(lr_size, Image.BICUBIC)\n",
    "\n",
    "    def get_transforms(self):\n",
    "        return {\n",
    "            'train': PairedTransform(self.train_transform),\n",
    "            'val': PairedTransform(self.val_transform)\n",
    "        }\n",
    "\n",
    "class PairedTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __call__(self, hr):\n",
    "        lr = self.transform(hr)\n",
    "        hr = self.transform(hr)\n",
    "        return lr, hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes\n",
    "\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=12,\n",
    "                 num_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(1, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 1 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:]\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 encoder_layer=12,\n",
    "                 encoder_head=3,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16_bn\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch*4, 3, padding=1),  # Pixel shuffle\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.res = ResidualBlock(out_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        return self.res(x)\n",
    "\n",
    "# class EnhancedDecoder(nn.Module):\n",
    "#     def __init__(self, emb_dim=192):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             nn.Conv2d(emb_dim, 256, 3, padding=1),\n",
    "#             ResidualBlock(256),\n",
    "#             UpSampleBlock(256, 128),  # 16x16 → 32x32\n",
    "#             UpSampleBlock(128, 64),   # 32x32 → 64x64\n",
    "#             UpSampleBlock(64, 32),    # 64x64 → 128x128\n",
    "#             nn.Conv2d(32, 3, 3, padding=1)\n",
    "#         )\n",
    "#         self.final_upscale = nn.Sequential(\n",
    "#             nn.Upsample(size=150, mode='bicubic', align_corners=False),\n",
    "#             # nn.Conv2d(3, 3, 3, padding=1)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.layers(x)        # Output: 128x128\n",
    "#         return self.final_upscale(x)  # → 150x150\n",
    "class EnhancedDecoder(nn.Module):\n",
    "    def __init__(self, emb_dim=192):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(emb_dim, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            ResidualBlock(256),\n",
    "            UpSampleBlock(256, 128),  # 16x16 → 32x32\n",
    "            UpSampleBlock(128, 64),   # 32x32 → 64x64\n",
    "            UpSampleBlock(64, 32),    # 64x64 → 128x128\n",
    "            nn.Conv2d(32, 1, 3, padding=1)  # Final conv to get single channel\n",
    "        )\n",
    "        \n",
    "        # Final upscale to 150x150\n",
    "        self.final_upscale = nn.Sequential(\n",
    "            nn.Upsample(size=150, mode='bicubic', align_corners=False),\n",
    "            nn.Conv2d(1, 1, 3, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.layers(x)\n",
    "        return self.final_upscale(x)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Loss Function Improvements\n",
    "# ---------------------------\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Remove VGG loss since we're working with single-channel images\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        return 0.5 * l1 + 0.5 * mse\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Input Adaptation\n",
    "# ---------------------------\n",
    "class SuperResolutionMAE(nn.Module):\n",
    "    def __init__(self, pretrained_mae):\n",
    "        super().__init__()\n",
    "        self.encoder = pretrained_mae.encoder\n",
    "        self.encoder.mask_ratio = 0\n",
    "        \n",
    "        # Adjust for 75x75 input via padding (75 → 96)\n",
    "        self.transform = nn.Sequential(\n",
    "            transforms.CenterCrop(64),  # First crop to 64x64\n",
    "            # nn.ReflectionPad2d((0, 0, 0, 0))  # No padding needed after crop\n",
    "        )  # 75→96\n",
    "        \n",
    "        self.decoder = EnhancedDecoder(emb_dim=192)\n",
    "        \n",
    "    def forward(self, lr_img):\n",
    "        # Pad 75x75 → 96x96 (divisible by MAE's patch size)\n",
    "        x = self.transform(lr_img)  # Now compatible with MAE's 4x4 patches\n",
    "        \n",
    "        # Extract features\n",
    "        features, _ = self.encoder(x)\n",
    "        # B = features.shape[1]\n",
    "        features = features[1:]  # Remove CLS token\n",
    "        \n",
    "        # Reshape to 2D feature map\n",
    "        num_patches = int(np.sqrt(features.shape[0]))\n",
    "        features = rearrange(features, '(h w) b c -> b c h w', h=num_patches, w=num_patches)\n",
    "        \n",
    "        # Decode\n",
    "        return self.decoder(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = MAE_ViT(\n",
    "        image_size=64,\n",
    "        patch_size=4,\n",
    "        emb_dim=192,\n",
    "        encoder_layer=12,\n",
    "        encoder_head=3,\n",
    "        decoder_layer=4,\n",
    "        decoder_head=3,\n",
    "        mask_ratio=0.75\n",
    ").to(device)\n",
    "mae.load_state_dict(torch.load('best_mae_model2.pth')['model_state_dict'])\n",
    "model = SuperResolutionMAE(mae).to(device)\n",
    "lr_img = torch.randn(1, 1, 75, 75).to(device)\n",
    "hr_pred = model(lr_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.encoder.patchify.weight[:, :, :4, :4].copy_(mae.encoder.patchify.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 150, 150])\n"
     ]
    }
   ],
   "source": [
    "print(hr_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, \n",
    "                 optimizer, criterion, device, config,\n",
    "                 scheduler=None, use_amp=True):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        self.scaler = GradScaler(device, enabled=use_amp)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.logger = WandBLogger(config, model)\n",
    "        self.log_interval = config.get('log_interval', 50)\n",
    "        self.sample_interval = config.get('sample_interval', 200)\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_epoch = 0 \n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        mse_values = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (lr, hr) in enumerate(self.train_loader):\n",
    "            lr = lr.to(self.device, non_blocking=True)\n",
    "            hr = hr.to(self.device, non_blocking=True)\n",
    "            # print(\"LR Shape: \",lr.shape)\n",
    "            # print(\"HR Shape: \",hr.shape)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            with autocast(enabled=self.scaler.is_enabled()):\n",
    "                outputs = self.model(lr)\n",
    "                # print(\"Output Shape: \",outputs.shape)\n",
    "                loss = self.criterion(outputs, hr)\n",
    "                \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(self.optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_mse = F.mse_loss(outputs, hr).item()\n",
    "            batch_psnr = calculate_psnr(outputs, hr)\n",
    "            batch_ssim = calculate_ssim(outputs, hr)\n",
    "            psnr_values.append(batch_psnr)\n",
    "            ssim_values.append(batch_ssim)\n",
    "            mse_values.append(batch_mse)\n",
    "\n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                self.logger.log_metrics({\n",
    "                    \"train/loss\": loss.item(),\n",
    "                    \"train/batch_mse\": batch_mse,\n",
    "                    \"train/batch_psnr\": batch_psnr,\n",
    "                    \"train/batch_ssim\": batch_ssim,\n",
    "                    \"lr\": self.optimizer.param_groups[0]['lr']\n",
    "                }, commit=False)\n",
    "                \n",
    "            if batch_idx % self.sample_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    self.logger.log_images(lr[:1], outputs[:1], hr[:1])\n",
    "                \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        avg_ssim = np.mean(ssim_values)\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        self.logger.log_metrics({\n",
    "            \"epoch\": epoch,\n",
    "            \"train/avg_loss\": avg_loss,\n",
    "            \"train/epoch_mse\": avg_mse,\n",
    "            \"train/avg_psnr\": avg_psnr,\n",
    "            \"train/avg_ssim\": avg_ssim,\n",
    "            \"epoch_time\": epoch_time\n",
    "        })\n",
    "        return avg_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self, epoch):\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        mse_values = []\n",
    "        psnr_values = []\n",
    "        ssim_values = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for lr, hr in self.val_loader:\n",
    "            lr = lr.to(self.device, non_blocking=True)\n",
    "            hr = hr.to(self.device, non_blocking=True)\n",
    "            \n",
    "            outputs = self.model(lr)\n",
    "            loss = self.criterion(outputs, hr)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            mse_values.append(F.mse_loss(outputs, hr).item())\n",
    "            psnr_values.append(calculate_psnr(outputs, hr))\n",
    "            ssim_values.append(calculate_ssim(outputs, hr))\n",
    "            \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        avg_psnr = np.mean(psnr_values)\n",
    "        avg_ssim = np.mean(ssim_values)\n",
    "        avg_mse = np.mean(mse_values)\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        self.logger.log_metrics({\n",
    "            \"val/loss\": avg_loss,\n",
    "            \"val/mse\": avg_mse,\n",
    "            \"val/psnr\": avg_psnr,\n",
    "            \"val/ssim\": avg_ssim,\n",
    "            \"epoch_time\": epoch_time\n",
    "        })\n",
    "\n",
    "        self.logger.log_metrics({\n",
    "            \"val_output_dist\": wandb.Histogram(outputs.cpu().numpy())\n",
    "        })\n",
    "\n",
    "        if avg_loss < self.best_val_loss:\n",
    "            self.best_val_loss = avg_loss\n",
    "            self.best_epoch = epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': self.model.state_dict(),\n",
    "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, 'equiformer_best.pth')\n",
    "            print(\"Saved best model!\")\n",
    "            self.logger.log_model('sr_mae_best.pth', {\n",
    "                'epoch': epoch,\n",
    "                'val_loss': avg_loss,\n",
    "                'val_psnr': avg_psnr\n",
    "            })\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step(avg_loss)\n",
    "            \n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    print(device)\n",
    "    mae = MAE_ViT(\n",
    "        image_size=64,\n",
    "        patch_size=4,\n",
    "        emb_dim=192,\n",
    "        encoder_layer=12,\n",
    "        encoder_head=3,\n",
    "        decoder_layer=4,\n",
    "        decoder_head=3,\n",
    "        mask_ratio=0\n",
    "    ).to(device)\n",
    "    mae.load_state_dict(torch.load('best_mae_model2.pth')['model_state_dict'])\n",
    "    model = SuperResolutionMAE(mae).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    criterion = CombinedLoss().to(device=device)\n",
    "    \n",
    "    # preprocessor = LensDataPreprocessor(crop_size=75)\n",
    "    # transforms = preprocessor.get_transforms()\n",
    "\n",
    "    train_dataset = LensDataset(\n",
    "        lr_dir=config['train_lr_dir'],\n",
    "        hr_dir=config['train_hr_dir'],\n",
    "        transform=config['transform']\n",
    "    )\n",
    "    \n",
    "    val_dataset = LensDataset(\n",
    "        lr_dir=config['val_lr_dir'],\n",
    "        hr_dir=config['val_hr_dir'],\n",
    "        transform=config['transform']\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        device=device,\n",
    "        scheduler=scheduler,\n",
    "        config=config,\n",
    "        use_amp=config['use_amp']\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        for epoch in range(1, config['epochs'] + 1):\n",
    "            train_loss = trainer.train_epoch(epoch)\n",
    "            val_loss = trainer.validate(epoch)\n",
    "\n",
    "            # Early stopping\n",
    "            if (epoch - trainer.best_epoch) > config['patience']:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "    finally:\n",
    "        trainer.logger.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ry/7v7x_96j307_y6kstfp40gcr0000gn/T/ipykernel_99841/1055358504.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = GradScaler(device, enabled=use_amp)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sshah/2024/projects/gsoc/ml4sci/fm/wandb/run-20250315_053210-7xncntle</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xncntle' target=\"_blank\">clear-yogurt-19</a></strong> to <a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational' target=\"_blank\">https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xncntle' target=\"_blank\">https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xncntle</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n",
      "/var/folders/ry/7v7x_96j307_y6kstfp40gcr0000gn/T/ipykernel_99841/1055358504.py:35: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=self.scaler.is_enabled()):\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">clear-yogurt-19</strong> at: <a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xncntle' target=\"_blank\">https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xncntle</a><br> View project at: <a href='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational' target=\"_blank\">https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250315_053210-7xncntle/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(config)\n",
      "Cell \u001b[0;32mIn[122], line 78\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_epoch(epoch)\n\u001b[1;32m     79\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mvalidate(epoch)\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;66;03m# Early stopping\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[121], line 36\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mis_enabled()):\n\u001b[0;32m---> 36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(lr)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print(\"Output Shape: \",outputs.shape)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, hr)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1848\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1791\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1792\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1794\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[117], line 123\u001b[0m, in \u001b[0;36mSuperResolutionMAE.forward\u001b[0;34m(self, lr_img)\u001b[0m\n\u001b[1;32m    120\u001b[0m features \u001b[38;5;241m=\u001b[39m rearrange(features, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(h w) b c -> b c h w\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mnum_patches, w\u001b[38;5;241m=\u001b[39mnum_patches)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(features)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1848\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1850\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1794\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1791\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1792\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1794\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1797\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[1;32m   1800\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[117], line 73\u001b[0m, in \u001b[0;36mEnhancedDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_conv(x)\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(x)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_upscale(x)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "train_model(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
