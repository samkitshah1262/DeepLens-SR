{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/homebrew/anaconda3/lib/python3.12/site-packages (0.8.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from einops import repeat, rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from timm.models.layers import trunc_normal_\n",
    "from timm.models.vision_transformer import Block\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import wandb\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/samkitshah1262-warner-bros-discovery/deeplens-foundational/runs/7xry5pnf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x33791c6b0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"deeplens-foundational\", entity=\"samkitshah1262-warner-bros-discovery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    SEED = 1\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 200\n",
    "    LEARNING_RATE = 1e-5\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    MASK_RATIO = 0.75\n",
    "    NUM_CLASSES = 3\n",
    "    IMG_SIZE = (64, 64)\n",
    "    USE_SAVED_MODEL = False\n",
    "    CKPT_PATH = \"./model_weights.pth\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(config.SEED)\n",
    "np.random.seed(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 64])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.load(\"dataset/no_sub/no_sub_sim_6956151560647865808482838248806684.npy\")  # Load .npy file\n",
    "img = torch.tensor(img, dtype=torch.float32)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_ROOT = \"/Users/sshah/2024/projects/gsoc/ml4sci/fm/Dataset\"\n",
    "DATA_PATH_AXION = \"/axion\"\n",
    "DATA_PATH_CDM = \"/cdm\"\n",
    "DATA_PATH_NO_SUB = \"/no_sub\"\n",
    "\n",
    "OUTPUT_ROOT = \"/dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NpyDirectoryDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        class_names = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "        class_names.sort()\n",
    "\n",
    "        for idx, class_name in enumerate(class_names):\n",
    "            self.class_to_idx[class_name] = idx\n",
    "\n",
    "        for class_name in class_names:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            class_idx = self.class_to_idx[class_name]\n",
    "            \n",
    "            for file_name in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, file_name)\n",
    "                if file_path.endswith('.npy') and os.path.isfile(file_path):\n",
    "                    self.samples.append((file_path, class_idx))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            data = np.load(file_path, allow_pickle=True)\n",
    "            img_array = data[0].astype(np.float32)\n",
    "            image = Image.fromarray(img_array)\n",
    "            image = image.point(lambda p: p / img_array.max())\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            return image, label\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            placeholder = torch.zeros((1, 64, 64))\n",
    "            return placeholder, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(root_dir, batch_size=32, train_ratio=0.8, num_workers=4, seed=42):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    full_dataset = NpyDirectoryDataset(root_dir=root_dir,transform=transforms)\n",
    "    \n",
    "    if len(full_dataset) == 0:\n",
    "        raise ValueError(f\"No valid .npy files found in {root_dir}. Please check the directory structure.\")\n",
    "    \n",
    "    print(f\"Found {len(full_dataset)} .npy files total\")\n",
    "    print(f\"Class mapping: {full_dataset.class_to_idx}\")\n",
    "\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    class_loaders = {}\n",
    "\n",
    "    class_names = list(full_dataset.class_to_idx.keys())\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        class_idx = full_dataset.class_to_idx[class_name]\n",
    "        class_indices = [i for i, (_, label) in enumerate(full_dataset.samples) if label == class_idx]\n",
    "        \n",
    "        if len(class_indices) == 0:\n",
    "            print(f\"Warning: No samples found for class {class_name}\")\n",
    "            continue\n",
    "\n",
    "        class_train_indices = [idx for idx in range(len(full_dataset)) if \n",
    "                              idx in train_dataset.indices and \n",
    "                              full_dataset.samples[idx][1] == class_idx]\n",
    "                              \n",
    "        class_val_indices = [idx for idx in range(len(full_dataset)) if \n",
    "                            idx in val_dataset.indices and \n",
    "                            full_dataset.samples[idx][1] == class_idx]\n",
    "        \n",
    "        class_train_subset = torch.utils.data.Subset(full_dataset, class_train_indices)\n",
    "        class_val_subset = torch.utils.data.Subset(full_dataset, class_val_indices)\n",
    "\n",
    "        class_loaders[f\"{class_name}_train\"] = DataLoader(\n",
    "            class_train_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        \n",
    "        class_loaders[f\"{class_name}_val\"] = DataLoader(\n",
    "            class_val_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "\n",
    "    loaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"val\": val_loader,\n",
    "        **class_loaders\n",
    "    }\n",
    "\n",
    "    print(\"\\nDataset information:\")\n",
    "    print(f\"Training set: {len(train_dataset)} samples\")\n",
    "    print(f\"Validation set: {len(val_dataset)} samples\")\n",
    "    \n",
    "    for name, loader in class_loaders.items():\n",
    "        print(f\"{name}: {len(loader.dataset)} samples\")\n",
    "    \n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89104 .npy files total\n",
      "Class mapping: {'axion': 0, 'cdm': 1, 'no_sub': 2}\n",
      "\n",
      "Dataset information:\n",
      "Training set: 80193 samples\n",
      "Validation set: 8911 samples\n",
      "axion_train: 26981 samples\n",
      "axion_val: 2915 samples\n",
      "cdm_train: 26723 samples\n",
      "cdm_val: 3036 samples\n",
      "no_sub_train: 26489 samples\n",
      "no_sub_val: 2960 samples\n"
     ]
    }
   ],
   "source": [
    "root_directory = DATA_PATH_ROOT\n",
    "\n",
    "loaders = create_data_loaders(\n",
    "    root_dir=root_directory,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    train_ratio=0.9,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 3\n",
      "train: 80193 samples, 1254 batches\n",
      "val: 8911 samples, 140 batches\n",
      "axion_train: 26981 samples, 422 batches\n",
      "axion_val: 2915 samples, 46 batches\n",
      "cdm_train: 26723 samples, 418 batches\n",
      "cdm_val: 3036 samples, 48 batches\n",
      "no_sub_train: 26489 samples, 414 batches\n",
      "no_sub_val: 2960 samples, 47 batches\n"
     ]
    }
   ],
   "source": [
    "axion_train_loader = loaders[\"axion_train\"]\n",
    "cdm_val_loader = loaders[\"cdm_val\"]\n",
    "combined_train_loader = loaders[\"train\"]\n",
    "\n",
    "print(f\"Number of classes: {len(loaders)//2 - 1}\")\n",
    "\n",
    "for name, loader in loaders.items():\n",
    "    print(f\"{name}: {len(loader.dataset)} samples, {len(loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_indexes(size : int):\n",
    "    forward_indexes = np.arange(size)\n",
    "    np.random.shuffle(forward_indexes)\n",
    "    backward_indexes = np.argsort(forward_indexes)\n",
    "    return forward_indexes, backward_indexes\n",
    "\n",
    "def take_indexes(sequences, indexes):\n",
    "    return torch.gather(sequences, 0, repeat(indexes, 't b -> t b c', c=sequences.shape[-1]))\n",
    "\n",
    "class PatchShuffle(torch.nn.Module):\n",
    "    def __init__(self, ratio) -> None:\n",
    "        super().__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def forward(self, patches : torch.Tensor):\n",
    "        T, B, C = patches.shape\n",
    "        remain_T = int(T * (1 - self.ratio))\n",
    "\n",
    "        indexes = [random_indexes(T) for _ in range(B)]\n",
    "        forward_indexes = torch.as_tensor(np.stack([i[0] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "        backward_indexes = torch.as_tensor(np.stack([i[1] for i in indexes], axis=-1), dtype=torch.long).to(patches.device)\n",
    "\n",
    "        patches = take_indexes(patches, forward_indexes)\n",
    "        patches = patches[:remain_T]\n",
    "\n",
    "        return patches, forward_indexes, backward_indexes\n",
    "\n",
    "class MAE_Encoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=12,\n",
    "                 num_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
    "        self.shuffle = PatchShuffle(mask_ratio)\n",
    "\n",
    "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        patches = patches + self.pos_embedding\n",
    "\n",
    "        patches, forward_indexes, backward_indexes = self.shuffle(patches)\n",
    "        print(f\"Shuffled patches: {patches.shape} | Stride: {patches.stride()}\")\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "\n",
    "        return features, backward_indexes\n",
    "\n",
    "class MAE_Decoder(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 num_layer=4,\n",
    "                 num_head=3,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
    "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
    "\n",
    "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head) for _ in range(num_layer)])\n",
    "\n",
    "        self.head = torch.nn.Linear(emb_dim, 3 * patch_size ** 2)\n",
    "        self.patch2img = Rearrange('(h w) b (c p1 p2) -> b c (h p1) (w p2)', p1=patch_size, p2=patch_size, h=image_size//patch_size)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        trunc_normal_(self.mask_token, std=.02)\n",
    "        trunc_normal_(self.pos_embedding, std=.02)\n",
    "\n",
    "    def forward(self, features, backward_indexes):\n",
    "        T = features.shape[0]\n",
    "        backward_indexes = torch.cat([torch.zeros(1, backward_indexes.shape[1]).to(backward_indexes), backward_indexes + 1], dim=0)\n",
    "        features = torch.cat([features, self.mask_token.expand(backward_indexes.shape[0] - features.shape[0], features.shape[1], -1)], dim=0)\n",
    "        features = take_indexes(features, backward_indexes)\n",
    "        features = features + self.pos_embedding\n",
    "\n",
    "        features = rearrange(features, 't b c -> b t c')\n",
    "        features = self.transformer(features)\n",
    "        features = rearrange(features, 'b t c -> t b c')\n",
    "        features = features[1:]\n",
    "\n",
    "        patches = self.head(features)\n",
    "        mask = torch.zeros_like(patches)\n",
    "        mask[T-1:] = 1\n",
    "        mask = take_indexes(mask, backward_indexes[1:] - 1)\n",
    "        img = self.patch2img(patches)\n",
    "        mask = self.patch2img(mask)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "class MAE_ViT(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size=64,\n",
    "                 patch_size=4,\n",
    "                 emb_dim=192,\n",
    "                 encoder_layer=12,\n",
    "                 encoder_head=3,\n",
    "                 decoder_layer=4,\n",
    "                 decoder_head=3,\n",
    "                 mask_ratio=0.75,\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, encoder_layer, encoder_head, mask_ratio)\n",
    "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, decoder_layer, decoder_head)\n",
    "\n",
    "    def forward(self, img):\n",
    "        features, backward_indexes = self.encoder(img)\n",
    "        predicted_img, mask = self.decoder(features,  backward_indexes)\n",
    "        return predicted_img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_loader = loaders['train']\n",
    "total_val_loader = loaders['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_Classifier(torch.nn.Module):\n",
    "    def __init__(self, encoder : MAE_Encoder, num_classes=3) -> None:\n",
    "        super().__init__()\n",
    "        encoder.mask_ratio = 0\n",
    "        self.cls_token = encoder.cls_token\n",
    "        self.pos_embedding = encoder.pos_embedding\n",
    "        self.patchify = encoder.patchify\n",
    "        self.transformer = encoder.transformer\n",
    "        self.layer_norm = encoder.layer_norm\n",
    "        self.head = torch.nn.Linear(self.pos_embedding.shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, img):\n",
    "        patches = self.patchify(img)\n",
    "        print(f\"Patch stride: {patches.stride()}, Contiguous: {patches.is_contiguous()}\")\n",
    "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
    "        print(f\"Rearranged stride: {patches.stride()}\")\n",
    "        patches = patches + self.pos_embedding\n",
    "        patches = torch.cat([self.cls_token.expand(-1, patches.shape[1], -1), patches], dim=0)\n",
    "        patches = rearrange(patches, 't b c -> b t c')\n",
    "        features = self.layer_norm(self.transformer(patches))\n",
    "        if not features.is_contiguous():\n",
    "            print(\"Warning: Non-contiguous features detected!\")  # [1]\n",
    "            features = features.contiguous()\n",
    "        logits = self.head(features[:, 0, :])\n",
    "        print(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "print(torch.load('best_mae_model2.pth').keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mae = MAE_ViT(\n",
    "        image_size=64,\n",
    "        patch_size=4,\n",
    "        emb_dim=192,\n",
    "        encoder_layer=12,\n",
    "        encoder_head=3,\n",
    "        decoder_layer=4,\n",
    "        decoder_head=3,\n",
    "        mask_ratio=0.75\n",
    ").to(device)\n",
    "mae.load_state_dict(torch.load('best_mae_model.pth')['model_state_dict'])\n",
    "model = ViT_Classifier(mae.encoder).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "def acc_fn(logit, label):\n",
    "    return torch.mean((logit.argmax(dim=-1) == label).float())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "def lr_func(epoch):\n",
    "    return min((epoch + 1) / (200 + 1e-8), 0.5 * (math.cos(epoch / config.EPOCHS * math.pi) + 1))\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_func, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc(model, loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels == i, probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    roc_auc[\"macro\"] = np.mean(list(roc_auc.values()))\n",
    "    \n",
    "    return fpr, tpr, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "def train_classifier(model, train_loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        images = images.repeat(1, 3, 1, 1)\n",
    "        print(\"Image shape before model:\", images.shape)   \n",
    "        images = images.clamp(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        print(f\"Model output shape: {outputs.shape}\")\n",
    "        print(f\"Labels shape: {labels.shape}\")  \n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(\"output cont: \",outputs.is_contiguous())\n",
    "        print(\"labels cont: \",labels.is_contiguous())\n",
    "        print(loss)    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    probs = torch.cat(all_probs).detach().to(\"cpu\").numpy()\n",
    "    labels = torch.cat(all_labels).detach().to(\"cpu\").numpy()\n",
    "\n",
    "    macro_auc = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    class_auc = roc_auc_score(labels, probs, multi_class='ovr', average=None)\n",
    "    \n",
    "    return macro_auc, class_auc , total_loss/len(train_loader.dataset)\n",
    "\n",
    "def validate_classifier(model, val_loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = images.repeat(1, 3, 1, 1)\n",
    "            images = images.clamp(0, 1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "    probs = torch.cat(all_probs).detach().to(\"cpu\").numpy()\n",
    "    labels = torch.cat(all_labels).detach().to(\"cpu\").numpy()\n",
    "\n",
    "    macro_auc = roc_auc_score(labels, probs, multi_class='ovr', average='macro')\n",
    "    class_auc = roc_auc_score(labels, probs, multi_class='ovr', average=None)\n",
    "    \n",
    "    return macro_auc, class_auc , total_loss/len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/1254 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape before model: torch.Size([64, 3, 64, 64])\n",
      "Patch stride: (49152, 256, 16, 1), Contiguous: True\n",
      "Rearranged stride: (1, 49152, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758],\n",
      "        [ 0.7127,  0.3803, -0.1758]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n",
      "Model output shape: torch.Size([64, 3])\n",
      "Labels shape: torch.Size([64])\n",
      "output cont:  True\n",
      "labels cont:  True\n",
      "tensor(1.1456, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(cnt\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m train_macro_auc, train_class_auc, train_loss \u001b[38;5;241m=\u001b[39m train_classifier(model, total_train_loader, optimizer, criterion, device, epoch)\n\u001b[1;32m     13\u001b[0m val_macro_auc, val_class_auc, val_loss \u001b[38;5;241m=\u001b[39m validate_classifier(model, total_val_loader, criterion, device, epoch)\n\u001b[1;32m     15\u001b[0m lr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[89], line 26\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(model, train_loader, optimizer, criterion, device, epoch)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels cont: \u001b[39m\u001b[38;5;124m\"\u001b[39m,labels\u001b[38;5;241m.\u001b[39mis_contiguous())\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)    \n\u001b[0;32m---> 26\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 624\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "best_auc = 0\n",
    "cnt = 0\n",
    "# wandb.watch(\n",
    "#     model,\n",
    "#     log='all',\n",
    "#     log_freq=50,\n",
    "#     log_graph=True\n",
    "# )\n",
    "for epoch in range(50):\n",
    "    if(cnt>10):\n",
    "        break\n",
    "    train_macro_auc, train_class_auc, train_loss = train_classifier(model, total_train_loader, optimizer, criterion, device, epoch)\n",
    "    val_macro_auc, val_class_auc, val_loss = validate_classifier(model, total_val_loader, criterion, device, epoch)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    print(f\"Epoch {epoch+1}/{config.EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_macro_auc\": train_macro_auc,\n",
    "        \"val_macro_auc\": val_macro_auc,\n",
    "        \"train_loss_cls\": train_loss,\n",
    "        \"val_loss_cls\": val_loss,\n",
    "        **{f\"train_class_{i}\": train_class_auc[i] for i in range(len(train_class_auc))},\n",
    "        **{f\"val_class_{i}\": val_class_auc[i] for i in range(len(val_class_auc))}\n",
    "    })\n",
    "\n",
    "    if val_macro_auc < best_auc:\n",
    "        best_auc = val_macro_auc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'auc': val_macro_auc,\n",
    "        }, \"best_mae_model_classifier.pth\")\n",
    "        print(f\"Saved new best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "    else:\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch stride: (49152, 256, 16, 1), Contiguous: True\n",
      "Rearranged stride: (1, 49152, 256)\n",
      "tensor([[-0.4271, -0.9497, -0.7466],\n",
      "        [-0.6088, -0.9777, -0.9135]], device='mps:0')\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 64, 64).to(device)  # Match input channels\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(out[0].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try SGD with momentum\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    momentum=0.9,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "# Add aggressive LR scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.5,  # Very high to force learning\n",
    "    steps_per_epoch=len(total_train_loader),\n",
    "    epochs=20,\n",
    "    div_factor=10,\n",
    "    final_div_factor=100\n",
    ")\n",
    "\n",
    "# # Add gradient clipping\n",
    "# torch.nn.utils.clip_grad_value_(model.parameters(), 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch stride: (49152, 256, 16, 1), Contiguous: True\n",
      "Rearranged stride: (1, 49152, 256)\n",
      "tensor([[ 0.5295,  0.4326, -0.5376],\n",
      "        [ 0.6002,  0.4888, -0.5002],\n",
      "        [ 0.6355,  0.5752, -0.4670],\n",
      "        [ 0.5756,  0.5411, -0.5141],\n",
      "        [ 0.5559,  0.5105, -0.4952],\n",
      "        [ 0.5530,  0.5286, -0.5353],\n",
      "        [ 0.5873,  0.4629, -0.4959],\n",
      "        [ 0.5475,  0.5014, -0.5208],\n",
      "        [ 0.5939,  0.4649, -0.5071],\n",
      "        [ 0.5330,  0.5722, -0.4829],\n",
      "        [ 0.5751,  0.5399, -0.5443],\n",
      "        [ 0.5976,  0.5495, -0.4857],\n",
      "        [ 0.5212,  0.5207, -0.5028],\n",
      "        [ 0.5961,  0.5154, -0.4940],\n",
      "        [ 0.5815,  0.5479, -0.4840],\n",
      "        [ 0.5778,  0.5628, -0.5304],\n",
      "        [ 0.5379,  0.5145, -0.5297],\n",
      "        [ 0.5786,  0.6069, -0.4967],\n",
      "        [ 0.5685,  0.5575, -0.4672],\n",
      "        [ 0.5874,  0.5011, -0.5251],\n",
      "        [ 0.5965,  0.4880, -0.4712],\n",
      "        [ 0.6031,  0.5097, -0.4804],\n",
      "        [ 0.5602,  0.5148, -0.5162],\n",
      "        [ 0.5873,  0.5368, -0.4902],\n",
      "        [ 0.5261,  0.5227, -0.5058],\n",
      "        [ 0.5957,  0.5039, -0.4869],\n",
      "        [ 0.5738,  0.4381, -0.5398],\n",
      "        [ 0.5727,  0.5042, -0.5418],\n",
      "        [ 0.5730,  0.4662, -0.5261],\n",
      "        [ 0.5948,  0.5117, -0.5125],\n",
      "        [ 0.5859,  0.6183, -0.5109],\n",
      "        [ 0.5664,  0.4958, -0.5008],\n",
      "        [ 0.5992,  0.5489, -0.5286],\n",
      "        [ 0.5624,  0.4984, -0.4932],\n",
      "        [ 0.5995,  0.5389, -0.4674],\n",
      "        [ 0.5634,  0.5184, -0.5160],\n",
      "        [ 0.5960,  0.5229, -0.4750],\n",
      "        [ 0.6153,  0.5559, -0.4607],\n",
      "        [ 0.6157,  0.5505, -0.4544],\n",
      "        [ 0.4969,  0.5069, -0.4702],\n",
      "        [ 0.5989,  0.5669, -0.4826],\n",
      "        [ 0.5701,  0.4905, -0.5165],\n",
      "        [ 0.6035,  0.5356, -0.5040],\n",
      "        [ 0.5389,  0.5613, -0.5031],\n",
      "        [ 0.5957,  0.4472, -0.4791],\n",
      "        [ 0.6047,  0.5367, -0.4806],\n",
      "        [ 0.6118,  0.5295, -0.4835],\n",
      "        [ 0.5549,  0.4559, -0.5047],\n",
      "        [ 0.5411,  0.5155, -0.4633],\n",
      "        [ 0.6069,  0.5063, -0.4821],\n",
      "        [ 0.5664,  0.5139, -0.4848],\n",
      "        [ 0.5893,  0.5336, -0.4981],\n",
      "        [ 0.6092,  0.4873, -0.4466],\n",
      "        [ 0.5663,  0.5594, -0.4671],\n",
      "        [ 0.5631,  0.5342, -0.5075],\n",
      "        [ 0.6119,  0.5438, -0.4737],\n",
      "        [ 0.5838,  0.5083, -0.4557],\n",
      "        [ 0.5701,  0.5294, -0.5079],\n",
      "        [ 0.6192,  0.4953, -0.5118],\n",
      "        [ 0.5964,  0.5910, -0.4850],\n",
      "        [ 0.5321,  0.4841, -0.4670],\n",
      "        [ 0.6299,  0.5801, -0.4626],\n",
      "        [ 0.6077,  0.5058, -0.5132],\n",
      "        [ 0.5305,  0.5733, -0.4805]], device='mps:0',\n",
      "       grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[0;32m----> 9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Print gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 624\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "x = torch.randn(64, 1, 64, 64).to(device)* 0.5 + 0.5  # Batch of random noise\n",
    "y = torch.randint(0, 1, (64,)).to(device)   # Random labels\n",
    "\n",
    "# Should see loss decrease rapidly if pipeline works\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Print gradients\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** 0.5\n",
    "    print(f\"Epoch {epoch}: Loss {loss.item():.4f}, Grad Norm {total_norm:.4f}\")\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature std: 1.0000218152999878\n",
      "Aligned feature std: 0.887792706489563\n",
      "Epoch 0: Loss 1.1467, Grad Norm 4.9957\n",
      "Epoch 1: Loss 1.2066, Grad Norm 6.9606\n",
      "Epoch 2: Loss 1.1566, Grad Norm 4.6473\n",
      "Epoch 3: Loss 1.1579, Grad Norm 5.1724\n",
      "Epoch 4: Loss 1.0509, Grad Norm 4.7101\n",
      "Epoch 5: Loss 1.0846, Grad Norm 3.8509\n",
      "Epoch 6: Loss 1.0984, Grad Norm 4.1500\n",
      "Epoch 7: Loss 1.1093, Grad Norm 6.1238\n",
      "Epoch 8: Loss 1.0519, Grad Norm 4.9738\n",
      "Epoch 9: Loss 1.0787, Grad Norm 3.8720\n"
     ]
    }
   ],
   "source": [
    "# class EnhancedClassifier(nn.Module):\n",
    "#     def __init__(self, encoder, num_classes=3):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.align = nn.Sequential(\n",
    "#             nn.Linear(192, 512),\n",
    "#             nn.LayerNorm(512),\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.5))\n",
    "#         self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         features, _ = self.encoder(x)\n",
    "#         aligned = self.align(features[0])  # CLS token\n",
    "#         return self.classifier(aligned)\n",
    "\n",
    "# mae = MAE_ViT(\n",
    "#         image_size=64,\n",
    "#         patch_size=4,\n",
    "#         emb_dim=192,\n",
    "#         encoder_layer=12,\n",
    "#         encoder_head=3,\n",
    "#         decoder_layer=4,\n",
    "#         decoder_head=3,\n",
    "#         mask_ratio=0.75\n",
    "# ).to(device)\n",
    "# mae.load_state_dict(torch.load('best_mae_model.pth')['model_state_dict'])\n",
    "# model = EnhancedClassifier(mae.encoder).to(device)\n",
    "# optimizer = torch.optim.AdamW([\n",
    "#     {'params': model.align.parameters(), 'lr': 1e-3},\n",
    "#     {'params': model.classifier.parameters(), 'lr': 1e-3},\n",
    "#     {'params': model.encoder.parameters(), 'lr': 1e-5}  # Slower encoder updates\n",
    "# ], weight_decay=0.01)\n",
    "\n",
    "# # Gradient clipping\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "# x = torch.randn(64, 3, 64, 64).to(device)* 0.5 + 0.5  # Batch of random noise\n",
    "# x = (x - x.mean()) / x.std()  # Standardize\n",
    "# x = torch.clamp(x, min=-3, max=3)\n",
    "# y = torch.randint(0, 3, (64,)).to(device)   # Random labels\n",
    "# with torch.no_grad():\n",
    "#     features = model.encoder(x)[0][0]\n",
    "#     aligned = model.align(features)\n",
    "\n",
    "# print(\"Original feature std:\", features.std().item())  # Should > 0.1\n",
    "# print(\"Aligned feature std:\", aligned.std().item())  \n",
    "# # Should see loss decrease rapidly if pipeline works\n",
    "# for epoch in range(10):\n",
    "#     optimizer.zero_grad()\n",
    "#     outputs = model(x)\n",
    "#     loss = criterion(outputs, y)\n",
    "#     loss.backward()\n",
    "    \n",
    "#     # Print gradients\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.detach().data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** 0.5\n",
    "#     print(f\"Epoch {epoch}: Loss {loss.item():.4f}, Grad Norm {total_norm:.4f}\")\n",
    "    \n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(original, reconstructed, mask_patch, patch_size=4):\n",
    "    \"\"\"\n",
    "    original: Tensor [B, C, H, W] (denormalized)\n",
    "    reconstructed: Tensor [B, C, H, W] (denormalized)\n",
    "    mask_patch: Tensor [B, N_patches] (1=masked)\n",
    "    \"\"\"\n",
    "    # Convert patch mask to pixel mask\n",
    "    B, C, H, W = original.shape\n",
    "    mask = mask_patch.view(B,1, H, W)\n",
    "    mask = mask.unsqueeze(-1).unsqueeze(-1)\n",
    "    mask = mask.repeat(1, 1, 1, patch_size, patch_size)\n",
    "    mask = mask.view(B, 1, H, W).float()\n",
    "    \n",
    "    # Calculate metrics only on masked regions\n",
    "    mse = ((original - reconstructed)**2 * mask).sum() / mask.sum()\n",
    "    \n",
    "    # PSNR\n",
    "    max_pixel = 1.0  # Assuming normalized to [0,1]\n",
    "    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n",
    "    \n",
    "    # SSIM (requires numpy conversion)\n",
    "    ssim_total = 0\n",
    "    for b in range(B):\n",
    "        orig_np = original[b].permute(1,2,0).numpy()\n",
    "        recon_np = reconstructed[b].permute(1,2,0).numpy()\n",
    "        mask_np = mask[b].squeeze().numpy()\n",
    "        \n",
    "        # SSIM on masked pixels only\n",
    "        ssim_val = ssim(orig_np, recon_np, \n",
    "                        data_range=1.0,\n",
    "                        multichannel=True,\n",
    "                        full=True,\n",
    "                        win_size=3,\n",
    "                        use_sample_covariance=False)[1]\n",
    "        ssim_masked = (ssim_val * mask_np).sum() / mask_np.sum()\n",
    "        ssim_total += ssim_masked\n",
    "        \n",
    "    return {\n",
    "        'mse': mse.item(),\n",
    "        'psnr': psnr.item(),\n",
    "        'ssim': ssim_total/B\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_reconstructions(original, masked, reconstructed, mask_patch, num_samples=3):\n",
    "    \"\"\"\n",
    "    original: Tensor [B, C, H, W]\n",
    "    masked: Tensor [B, C, H, W]\n",
    "    reconstructed: Tensor [B, C, H, W]\n",
    "    mask_patch: Tensor [B, N_patches]\n",
    "    \"\"\"\n",
    "    patch_size = 4\n",
    "    B, C, H, W = original.shape\n",
    "    \n",
    "    fig, axs = plt.subplots(num_samples, 4, figsize=(15, num_samples*3))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Original image\n",
    "        axs[i,0].imshow(original[i].permute(1,2,0))\n",
    "        axs[i,0].set_title('Original')\n",
    "        axs[i,0].axis('off')\n",
    "        \n",
    "        # Mask visualization\n",
    "        patch_mask = mask_patch[i].view(H//patch_size, W//patch_size)\n",
    "        axs[i,1].imshow(patch_mask, cmap='gray')\n",
    "        axs[i,1].set_title('Patch Mask')\n",
    "        axs[i,1].axis('off')\n",
    "        \n",
    "        # Masked input\n",
    "        axs[i,2].imshow(masked[i].permute(1,2,0))\n",
    "        axs[i,2].set_title('Masked Input')\n",
    "        axs[i,2].axis('off')\n",
    "        \n",
    "        # Reconstruction\n",
    "        axs[i,3].imshow(reconstructed[i].permute(1,2,0))\n",
    "        axs[i,3].set_title('Reconstruction')\n",
    "        axs[i,3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_mae(model, dataloader, device):\n",
    "    model.eval()\n",
    "    metrics = {'mse': [], 'psnr': [], 'ssim': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Forward pass with mask return\n",
    "            reconstructed, pixel_mask, patch_mask = model(images)\n",
    "            \n",
    "            # Denormalize\n",
    "            original = (images * 0.5) + 0.5  # Assuming normalization was (mean=0.5, std=0.5)\n",
    "            reconstructed = (reconstructed * 0.5) + 0.5\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_metrics = calculate_metrics(\n",
    "                original.cpu(),\n",
    "                reconstructed.cpu(),\n",
    "                patch_mask.cpu()\n",
    "            )\n",
    "            \n",
    "            # Store metrics\n",
    "            for k in metrics:\n",
    "                metrics[k].append(batch_metrics[k])\n",
    "            \n",
    "            # Visualize first batch\n",
    "            if len(metrics['mse']) == 1:\n",
    "                masked_input = original * (1 - patch_mask.unsqueeze(1))  # Create masked input\n",
    "                visualize_reconstructions(\n",
    "                    original.cpu(),\n",
    "                    masked_input.cpu(),\n",
    "                    reconstructed.cpu(),\n",
    "                    patch_mask.cpu()\n",
    "                )\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    final_metrics = {k: np.mean(v) for k,v in metrics.items()}\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    print(f\"MSE: {final_metrics['mse']:.4f}\")\n",
    "    print(f\"PSNR: {final_metrics['psnr']:.2f} dB\")\n",
    "    print(f\"SSIM: {final_metrics['ssim']:.4f}\")\n",
    "    \n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MAE_ViT' object has no attribute 'image_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m loaders[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Run verification\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m metrics \u001b[38;5;241m=\u001b[39m verify_mae(mae, test_loader, device)\n",
      "Cell \u001b[0;32mIn[27], line 10\u001b[0m, in \u001b[0;36mverify_mae\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Forward pass with mask return\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m reconstructed, pixel_mask, patch_mask \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Denormalize\u001b[39;00m\n\u001b[1;32m     13\u001b[0m original \u001b[38;5;241m=\u001b[39m (images \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m  \u001b[38;5;66;03m# Assuming normalization was (mean=0.5, std=0.5)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 137\u001b[0m, in \u001b[0;36mMAE_ViT.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    134\u001b[0m predicted_img, pixel_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(features, backward_indexes)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Create patch-level mask\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m H \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[1;32m    138\u001b[0m patch_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(img\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), H\u001b[38;5;241m*\u001b[39mW)\u001b[38;5;241m.\u001b[39mto(img\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(backward_indexes):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1935\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1934\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1935\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1937\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MAE_ViT' object has no attribute 'image_size'"
     ]
    }
   ],
   "source": [
    "mae = MAE_ViT(\n",
    "        image_size=64,\n",
    "        patch_size=4,\n",
    "        emb_dim=192,\n",
    "        encoder_layer=12,\n",
    "        encoder_head=3,\n",
    "        decoder_layer=4,\n",
    "        decoder_head=3,\n",
    "        mask_ratio=0.75\n",
    ").to(device)\n",
    "mae.load_state_dict(torch.load('best_mae_model.pth')['model_state_dict'])\n",
    "# Create test dataloader with proper normalization\n",
    "\n",
    "test_loader = loaders['val']\n",
    "# Run verification\n",
    "metrics = verify_mae(mae, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m      6\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m----> 7\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Log gradients\u001b[39;00m\n\u001b[1;32m     10\u001b[0m total_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    623\u001b[0m     )\n\u001b[0;32m--> 624\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Add gradient monitoring\n",
    "for batch_idx, (data, target) in enumerate(total_train_loader):\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Log gradients\n",
    "    total_grad = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_mean = param.grad.abs().mean().item()\n",
    "            total_grad += grad_mean\n",
    "    print(f\"Batch {batch_idx} - Avg grad magnitude: {total_grad/len(list(model.parameters())):.6f}\")\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_roc(model, loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.cuda()\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    probs = torch.cat(all_probs).numpy()\n",
    "    labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "    wandb_roc_data = []\n",
    "\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels == i, probs[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        # Store data for wandb\n",
    "        for j in range(len(fpr[i])):\n",
    "            wandb_roc_data.append([fpr[i][j], tpr[i][j], f'Class {i}'])\n",
    "\n",
    "    roc_auc[\"macro\"] = np.mean(list(roc_auc.values()))\n",
    "\n",
    "    # Log ROC curve to wandb\n",
    "    wandb.log({\"roc_curve\": wandb.Table(data=wandb_roc_data, columns=[\"FPR\", \"TPR\", \"Class\"])})\n",
    "\n",
    "    # Log AUC values\n",
    "    wandb.log({f\"AUC_Class_{i}_final\": roc_auc[i] for i in range(3)})\n",
    "    wandb.log({\"AUC_Macro_final\": roc_auc[\"macro\"]})\n",
    "\n",
    "    return fpr, tpr, roc_auc\n",
    "\n",
    "fpr, tpr, roc_auc = evaluate_roc(mae, total_val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
